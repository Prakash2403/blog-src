---
title: "Learning Partial Differential Equations From Data Using Neural Networks"
date: 2020-01-21T06:13:50+05:30
post-style: ""
tags: ['AI', 'Mathematics', '5-Min-Read']
domain: ['AI']
markup: "mmark"
summary: "Given a set of dictionary functiWons and noisy data."
---

#### Key definitions

* Dictionary functions:

### Prerequisites

A PDE can be represented as follows:

  $$F(\textbf{x}, f(\textbf{x}),  \frac{\partial f}{\partial \textbf{x}_1}, \frac{\partial f}{\partial \textbf{x}_2}...\frac{\partial ^t f}{\partial \textbf{x}_n^t}) = 0$$

If one restricts F to be linear combination of some pre-defined dictionary functions, then above equation can be represented as

  $$ \textbf{D} \phi = \sum_{i=0}^{k}\phi_iD_i = 0 $$

where D is dictionary of functions and $$\phi$$ is vector containing linear coefficients. It is evident that $$\phi \medspace \epsilon \medspace N(D)$$ where $$N(D)$$ is null space of D.

A point to note here is: $$\textbf{Null space is spanned by right singular vectors whose associated singular value is 0.}$$

### Available Data

Three things are available to us

$$\textbf{X}, f(\textbf{X}), \textbf{D}$$

where,

 $$\textbf{X}=(x_1, x_2, x_3......x_n)$$ and $$x_i \medspace \epsilon \medspace \R^d $$

 $$f(\textbf{X})=(f(x_1), f(x_2), f(x_3)......f(x_n))$$ and $$ f(x_i) \medspace \epsilon \medspace \R$$

 $$\textbf{D} = (D_1, D_2, D_3.........D_k)$$ ; $$D_i$$ is a dictionary function

### Output we want

* An approximation of $$f$$
* Underlying PDE as linear combination of dictionary functions

### Summary of the paper

#### How PDEs will be used for parameter estimation
Since, Neural Networks are universal function approximators(under certain assumptions, of course), one can use them to approximate
$$\textbf{f(x)}$$. Let's call the approximated function $$\bar f$$

Only issue with this approach is, it overfits if data is noisy.

And here is where PDE shines. We add a regularizer term which adds a penalty if the function generated by neural network isn't a solution to PDE (We'll discuss the penalization aspect later).

But we still haven't figured out how to come up with $$\phi$$. So lets focus on that.

#### FInding $$\phi$$

The approach is as follows:

1. Sample $$\textbf{k}$$ elements from $$\textbf{x}$$. Call the set of points $$\bar \textbf{x}$$
2. Evaluate dictionary functions $$D$$ on $$\bar \textbf{x}$$. This operation will be represented as $$D(\bar f, \bar x)$$
3. If there were $$L$$ dictionary functions, then step (2) will yield a matrix of dimension $$k \times L$$. Let's call the matrix $$\bar K$$
4. Using _some technique_, find null vectors of $$\bar K$$. Any null vector of $$\bar K$$ is capable of being $$\phi$$. Use some other regularization to choose one.

But there is a small catch here. In step 2, we are evaluating dictionary functions on sampled data points. This has one issue: We don't know exact representation of $$\textbf{f}$$. We are relying on our neural network to give us an approximate representation of $$\textbf{f}$$ and while neural networks are theoretically capable of approximating a function(again, under few assumptions) to arbitrary accuracy, they can't produce the exact representation. So, the matrix $$\bar K$$ has some _implicit_ error which can't be removed. This also means that $$\bar K$$ may be a full-rank matrix(because of independent _implicit_ error). Hence, we may not find any null vector in step (4).

But, if one assumes that every dictionary function is distributive and $$|f-\bar f| < \epsilon$$, where $$\epsilon$$ is an arbitrary positive number, then one can easily prove that $$\bar K$$ will approximate $$D(f, \bar x)$$, where $$D(f, \bar x)$$ denotes dictionary function being evaluated on $$\bar x$$, using the exact representation of $$f$$. Thus, the right singular vector associated with smallest singular of $$\bar K$$ will be an approximation for $$\phi$$ (Remember, SVD always gives non-negative eigenvalues, and null space is spanned by right singular vectors whose associated singular value is 0. So, if $$\bar K$$ is an approximation to $$D(f, \bar x)$$, then right singular vector corresponding to singular value closest to 0(i.e. minimum singular value) will be an approximation to $$\phi$$)

Finally, we know what to find and why to find it. We just don't know how. There are several methods of finding smallest eigenvalue and corresponding eigenvector for a given matrix. Authors of this paper choose to go with [_Min-Max theorem for Singular values_](https://en.wikipedia.org/wiki/Min-max_theorem#Min-max_principle_for_singular_values). Based on quality and quantity of your data, you may choose to go with some other method.

#### Formulating loss function

We want our approximated function to fit the data as well as Underlying PDEs. This means our loss fucnction will have atleast two _parts_. And since it's Machine Learning, it won't be complete without _Occam's Razor_. Hence, we can say that

$$L_{net} = L_{data\_fitting} + L_{PDE\_fitting} + L_{simplicity} $$

Assuming data points($$\textbf{X}$$) are I.I.D. and coming from a Guassian distribution, $$L_{data\_fitting}$$ optimal loss function is simply MSE. In most practical cases, people tend to ignore the Gaussian distribution prior assumption(never understood why they do it. Humans are complicated, aren't they?)

$$L_{PDE\_fitting}$$ is also simple.  

SVD: Get last col val | Calculate L2 norm of D\phi| That will be your loss
